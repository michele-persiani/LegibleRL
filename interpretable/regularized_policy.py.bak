import numpy as np
from scipy.stats import norm
from scipy.special import softmax


def boltzmann(values, tau, clip=(-7e2,7e2)):
    values = np.array(values, dtype='float64')
    if len(values.shape) == 1:
        values = np.expand_dims(values, 0)
    v = np.exp(np.clip(values / tau, clip[0], clip[1]))
    v /= np.sum(v, axis=1, keepdims=True)
    return v


def egreedy(values, tau):
    ids_max = np.max(values, axis=1, keepdims=True) == values
    e = tau / (values.shape[1]-1)
    p_a = np.zeros_like(values)
    p_a[ids_max] = 1 - tau
    p_a[~ids_max] = e
    return p_a


class RegularizedPolicy:

    def __init__(self, q_agents, num_actions, policy_fcn_regul, policy_fcn_result):
        """

        :param q_agents: list of N agents. agent(state) should return ar array of 'num_actions' values
        :param num_actions: number of actions of the agents
        :param policy_fcn_regul: policy function used for regularization. policy_fcn(q_values) return an array of probabilities
        :param policy_fcn_result: policy function used for selecting actions from the regularized Q-values. policy_fcn(q_values) return an array of probabilities
        """
        self.agents = q_agents
        self.policy_fcn_regul = policy_fcn_regul
        self.policy_fcn_result = policy_fcn_result
        self.num_actions = num_actions


    def create_masks(self, state):
        """
        Create the masks for computing interpretability
        :param state: the state that will later be masked
        :return: a numpy array (N,...) with all of the N masks stacked
        """
        raise NotImplementedError


    def apply_mask(self, state, mask):
        """
        Apply a mask to the given state
        :param state: a numpy array for the state
        :param mask: a numpy array for the mask
        :return: a numpy array for the masked state of the same dimensions of 'state'
        """
        raise NotImplementedError


    def mask_loss(self, mask):
        """
        Computes the cost of using a given mask
        :param mask:
        :return:
        """
        return np.sum(mask > 0)

    @property
    def num_policies(self):
        return len(self.agents)

    def get_q_values(self, state):
        """
        Computes the Q-values from all the considered agents
        :param state:
        :return: An array (N,A) where N in the number of agents and A the number of actions
        """
        q = [ag(state) for ag in self.agents]
        q = np.vstack(q)
        return q


    def find_masks_q_values(self, state, base_policy, regul_factor):
        """

        :param state:
        :param base_policy: int. the integer identifying which is the policy being regularized among those available
        :param regul_factor: float. magnitude of the regularization
        :return: Two numpy arrays. The first is the Q-values for every mask, the second is the masks
        """
        #---


        masks = self.create_masks(state)


        q_values_orig = np.zeros((len(masks), self.num_policies, self.num_actions))
        q_regul = np.zeros((len(masks), self.num_policies, self.num_actions))
        q_values = np.zeros((len(masks), self.num_actions))
        p_a_masks = np.zeros((len(masks), self.num_actions))
        for i, m in enumerate(masks):
            masked_state = self.apply_mask(state, m)
            masked_q_values = self.get_q_values(masked_state)

            regul_loss = self.legibility_loss(masked_state, base_policy, mask=m)


            q_values_regul =  masked_q_values + regul_factor * regul_loss[base_policy,:]

            q_values_orig[i,:] = masked_q_values
            q_regul[i,:] = -regul_loss
            q_values[i, :] = q_values_regul[base_policy, :]
            p_a_masks[i,:] = self.policy_fcn_regul(q_values[i, :])

        #-----

        return q_values, p_a_masks, masks




    def legibility_loss(self, state, base_policy, mask=None):
        """

        :param state:
        :param mask: mask to apply to the state before computing the loss
        """

        if mask is not None:
            state = self.apply_mask(state, mask)
        q_values = self.get_q_values(state)
        p_a = self.policy_fcn_regul(q_values)
        log_p_a = np.log(p_a)
        log_sum_p_a = np.log(np.sum(p_a, axis=0, keepdims=True))

        regul_loss =  - log_p_a + log_sum_p_a
        regul_loss *= -1
        return regul_loss

    def _action_proba(self, q_values):
        assert len(q_values.shape) == 3
        return self.policy_fcn_regul(q_values.reshape(-1, self.num_actions)).reshape(-1, self.num_policies, self.num_actions)


    def compute_explanation(self, state, a, base_policy, mask_regul):
        e_masks = self.create_masks(state)
        masks_costs = np.array([mask_regul * self.mask_loss(m) for m in e_masks])

        # Unregularized policy
        q_values = self.get_q_values(state)
        p_a = self.policy_fcn_regul(q_values)


        m0 = self.find_masks(e_masks, state, 0).reshape(-1,1)
        m1 = self.find_masks(e_masks, state, 1).reshape(-1,1)

        # Masks scores
        p_m = self.compute_masked_q(state, e_masks, p_a, False)


        # 1st order explanations
        p_a_a = self.policy_fcn_regul(q_values)
        num = p_m * p_a_a
        p_pi_m = np.sum(num, -1) / np.sum(np.sum(num, 1, keepdims=True), -1)
        p_pi_m_a = num / np.sum(num, 1, keepdims=True)


        p_pi = p_pi_m_a
        s = p_pi[:, base_policy, a].copy()
        mean, std = norm.fit(s)
        std = np.maximum(std, 1e-16)

        #p_values = 2 * np.minimum(1 - norm.cdf(s.flatten(), mean, std), norm.cdf(s.flatten(), mean, std))
        p_values = 1 - norm.cdf(s.flatten(), mean, std)

        s = (1-p_values) > 0.6


        r_mask = np.sum(e_masks * np.expand_dims(s, (1,2)), 0)


        #r_mask = (r_mask - np.min(r_mask)) / (np.max(r_mask) - np.min(r_mask) + 1e-4)
        if np.any(np.isnan(r_mask)):
            print('nan computing masks')


        return r_mask

    def select_action(self, state, base_policy, regul_factor):

        # Unregularized policy
        q_values = self.get_q_values(state)
        p_a = self.policy_fcn_regul(q_values)

        #--- Legible policy
        p_a_regul = self.policy_fcn_regul(q_values)
        p_pi_regul = p_a_regul / np.sum(p_a_regul, 0, keepdims=True)
        q_values_regul = q_values + regul_factor * np.log(p_pi_regul)
        p_a_regul = self.policy_fcn_result(q_values_regul)


        #--- P(a|s) of legible policy
        a = np.random.choice(3, p=p_a_regul[base_policy, :])
        return a


# ---------

    def find_masks(self, masks, state, color):
        s = state['colors'][:, :, color]
        s = np.sum(masks * s, axis=(1,2))
        return s > 0


    def compute_masked_q(self, state, masks, p_pi, sum_a=False):
        q_values = np.zeros((len(masks), self.num_policies, self.num_actions))
        q_values_p = np.zeros((len(masks), self.num_policies, self.num_actions))
        for i, m in enumerate(masks):
            masked_state = self.apply_mask(state, (1-m))
            q_values[i, :]   = self.get_q_values(state)
            q_values_p[i, :] = self.get_q_values(masked_state)


        scores = self.q_scores(q_values, q_values_p)#1e1*self.dkl_scores(q_values, q_values_p)#
        scores /= scores.sum()

        p_pi = np.expand_dims(p_pi, 0)
        scores = scores * np.log(scores / p_pi)


        p_m_pi_a = softmax(1e1*scores, 0)
        p_pi_m_a = softmax(1e1*scores, 1)




        #p_m_pi_a = scores / scores.sum(0, keepdims=True)
        #p_a_m = scores / scores.sum(-1, keepdims=True)

        #p_pi_a_m = p_a_m / p_a_m.sum(1, keepdims=True)
        #p_m_pi_a = p_a_m / p_a_m.sum(0, keepdims=True)

        if np.any(np.isnan(p_m_pi_a)):
            print('nan computing masks scores')
        return p_m_pi_a


    def q_scores(self, q_v, q_v_m):
        s = (q_v - q_v_m)**2
        s -= np.max(s)
        s = np.exp(s)
        s += 1e-6
        return s

    def dkl_scores(self, q_v, q_v_m):
        p_a = self._action_proba(q_v)
        p_a_r = self._action_proba(q_v_m)

        p_diff = (p_a - p_a_r)#[:,:, sel_a]

        dkl = p_a * np.log(p_a/p_a_r)
        return dkl

        K = []
        for i in range(self.num_actions):
            d = np.copy(dkl)
            d[:,:,i] = 0
            k = 1 / (1 + 2*np.sum(d, -1))
            K += np.expand_dims(k, -1),
        K = np.concatenate(K, -1)

        scores = -2 * K * p_diff / (K + p_diff)
        scores -= np.max(scores)
        return scores